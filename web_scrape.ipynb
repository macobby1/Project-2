{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#button_next = soup.find(\"a\", {\"class\": \"btn paging-next\"}, href=True)\n",
    "#while button_next:\n",
    "#    time.sleep(2)#delay time requests are sent so we don\\'t get kicked by server\n",
    "#    url2 = \"https://www.propertypal.com{0}\".format(button_next[\"href\"])\n",
    "#    page2=requests.get(url2)\n",
    "#    print(url2)\n",
    "#    soup=BeautifulSoup(page2.text,\"lxml\")\n",
    "#    g_data = soup.findAll(\"div\", {\"class\": \"propbox-details\"})\n",
    "#    for item in g_data:\n",
    "#        try:\n",
    "#            title = item.find_all(\"span\", {\"class\": \"propbox-addr\"})[0].text\n",
    "#        except:\n",
    "#            pass\n",
    "#       try:\n",
    "#            town = item.find_all(\"span\", {\"class\": \"propbox-town\"})[0].text\n",
    "#        except:\n",
    "#            pass\n",
    "#        try:\n",
    "#            price = item.find_all(\"span\", {\"class\": \"price-value\"})[0].text\n",
    "#        except:\n",
    "#            pass\n",
    "#        try:\n",
    "#            period = item.find_all(\"span\", {\"class\": \"price-period\"})[0].text\n",
    "#        except:\n",
    "#            pass\n",
    "\n",
    "#    course=[title,town,price,period]\n",
    "#    houses.append(course)\n",
    "\n",
    "#    button_next = soup.find(\"a\", {\"class\": \"btn paging-next\"}, href=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dependencies\n",
    "import pandas as pd\n",
    "import requests as req\n",
    "import time\n",
    "from datetime import date\n",
    "import datetime as dt\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import re\n",
    "from splinter import Browser\n",
    "from selenium import webdriver\n",
    "from splinter.exceptions import ElementDoesNotExist\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver import ActionChains\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.action_chains import ActionChains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "which: no chromedriver in (/c/Users/clayt/AppData/Roaming/Python/Python37/site-packages/pywin32_system32:/c/Users/clayt/AppData/Roaming/Python/Python37/site-packages/pywin32_system32:/c/Users/clayt/AppData/Roaming/Python/Python37/site-packages/pywin32_system32:/c/Users/clayt/bin:/mingw64/bin:/usr/local/bin:/usr/bin:/usr/bin:/mingw64/bin:/usr/bin:/c/Users/clayt/bin:/c/windows/system32:/c/windows:/c/windows/System32/Wbem:/c/windows/System32/WindowsPowerShell/v1.0:/c/windows/System32/OpenSSH:/c/Program Files (x86)/NVIDIA Corporation/PhysX/Common:/c/Program Files/NVIDIA Corporation/NVIDIA NvDLISR:/cmd:/c/Users/clayt/Anaconda3:/c/Users/clayt/Anaconda3/Library/mingw-w64/bin:/c/Users/clayt/Anaconda3/Library/usr/bin:/c/Users/clayt/Anaconda3/Library/bin:/c/Users/clayt/Anaconda3/Scripts:/c/Users/clayt/AppData/Local/Microsoft/WindowsApps:/c/Users/clayt/AppData/Local/Programs/Microsoft VS Code/bin:/c/Users/clayt/AppData/Local/atom/bin:/c/Users/clayt/AppData/Local/GitHubDesktop/bin:/c/Program Files/heroku/bin:/usr/bin/vendor_perl:/usr/bin/core_perl)\n"
     ]
    }
   ],
   "source": [
    "#https://splinter.readthedocs.io/en/latest/drivers/chrome.html\n",
    "!which chromedriver\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set the path for chromedriver\n",
    "executable_path = {'executable_path': 'chromedriver'}\n",
    "browser = Browser('chrome', **executable_path, headless=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visit url \n",
    "news_url = \"https://www.washingtonpost.com/graphics/politics/trump-claims-database/js/base.js?c=2754871e7eff8a3208c4fe127e3e4bcb01ada41a-1590961789\"\n",
    "browser.visit(news_url)\n",
    "html = browser.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Scrape\n",
      "50\n",
      "Scrape Complete\n",
      "Starting Scrape\n",
      "0\n",
      "Scrape Complete\n",
      "Starting Scrape\n",
      "50\n",
      "Scrape Complete\n",
      "Starting Scrape\n",
      "50\n",
      "Scrape Complete\n",
      "Starting Scrape\n",
      "50\n",
      "Scrape Complete\n",
      "Starting Scrape\n",
      "50\n",
      "Scrape Complete\n",
      "Starting Scrape\n",
      "50\n",
      "Scrape Complete\n",
      "Starting Scrape\n",
      "50\n",
      "Scrape Complete\n",
      "Starting Scrape\n",
      "50\n",
      "Scrape Complete\n",
      "Starting Scrape\n",
      "50\n",
      "Scrape Complete\n",
      "Starting Scrape\n",
      "50\n",
      "Scrape Complete\n",
      "Starting Scrape\n",
      "50\n",
      "Scrape Complete\n",
      "Starting Scrape\n",
      "50\n",
      "Scrape Complete\n",
      "Starting Scrape\n",
      "50\n",
      "Scrape Complete\n",
      "Starting Scrape\n",
      "50\n",
      "Scrape Complete\n",
      "Starting Scrape\n",
      "50\n",
      "Scrape Complete\n",
      "Starting Scrape\n",
      "50\n",
      "Scrape Complete\n",
      "Starting Scrape\n",
      "50\n",
      "Scrape Complete\n",
      "Starting Scrape\n",
      "50\n",
      "Scrape Complete\n",
      "Starting Scrape\n",
      "50\n",
      "Scrape Complete\n",
      "Starting Scrape\n",
      "50\n",
      "Scrape Complete\n",
      "Starting Scrape\n",
      "50\n",
      "Scrape Complete\n",
      "Starting Scrape\n",
      "50\n",
      "Scrape Complete\n",
      "Starting Scrape\n",
      "50\n",
      "Scrape Complete\n",
      "Starting Scrape\n",
      "50\n",
      "Scrape Complete\n",
      "Starting Scrape\n",
      "50\n",
      "Scrape Complete\n",
      "Starting Scrape\n",
      "50\n",
      "Scrape Complete\n",
      "Starting Scrape\n",
      "50\n",
      "Scrape Complete\n",
      "Starting Scrape\n",
      "50\n",
      "Scrape Complete\n",
      "Starting Scrape\n",
      "50\n",
      "Scrape Complete\n",
      "Starting Scrape\n",
      "50\n",
      "Scrape Complete\n",
      "Starting Scrape\n",
      "50\n",
      "Scrape Complete\n",
      "Starting Scrape\n",
      "50\n",
      "Scrape Complete\n",
      "Starting Scrape\n",
      "50\n",
      "Scrape Complete\n",
      "Starting Scrape\n",
      "50\n",
      "Scrape Complete\n",
      "Starting Scrape\n",
      "50\n",
      "Scrape Complete\n",
      "Starting Scrape\n",
      "50\n",
      "Scrape Complete\n",
      "Starting Scrape\n",
      "50\n",
      "Scrape Complete\n",
      "Starting Scrape\n",
      "50\n",
      "Scrape Complete\n",
      "Starting Scrape\n",
      "50\n",
      "Scrape Complete\n",
      "Starting Scrape\n",
      "50\n",
      "Scrape Complete\n",
      "Starting Scrape\n",
      "50\n",
      "Scrape Complete\n",
      "Starting Scrape\n",
      "50\n",
      "Scrape Complete\n",
      "Starting Scrape\n",
      "50\n",
      "Scrape Complete\n",
      "Starting Scrape\n",
      "50\n",
      "Scrape Complete\n",
      "Starting Scrape\n",
      "50\n",
      "Scrape Complete\n",
      "Starting Scrape\n",
      "50\n",
      "Scrape Complete\n",
      "Starting Scrape\n",
      "50\n",
      "Scrape Complete\n",
      "Starting Scrape\n",
      "50\n",
      "Scrape Complete\n",
      "Starting Scrape\n",
      "50\n",
      "Scrape Complete\n",
      "Starting Scrape\n",
      "50\n",
      "Scrape Complete\n",
      "Starting Scrape\n",
      "50\n",
      "Scrape Complete\n",
      "Starting Scrape\n",
      "50\n",
      "Scrape Complete\n",
      "Starting Scrape\n",
      "50\n",
      "Scrape Complete\n",
      "Starting Scrape\n",
      "50\n",
      "Scrape Complete\n",
      "Starting Scrape\n",
      "50\n",
      "Scrape Complete\n",
      "Starting Scrape\n",
      "50\n",
      "Scrape Complete\n",
      "Starting Scrape\n"
     ]
    }
   ],
   "source": [
    "##################################################################################################################\n",
    "\n",
    "from splinter import Browser\n",
    "import time\n",
    "from bs4 import BeautifulSoup as bs\n",
    "count=0\n",
    "raw_claims_list = []\n",
    "while len(raw_claims_list) < 1000:\n",
    "    print(\"Starting Scrape\")   \n",
    "    #click the \"load more claims button until we reach the end of the data\"\n",
    "    executable_path = {'executable_path': 'chromedriver'}\n",
    "    browser = Browser('chrome', **executable_path, headless=False)\n",
    "\n",
    "    browser.visit(\"https://www.washingtonpost.com/graphics/politics/trump-claims-database/\")\n",
    "    count=0\n",
    "    its_there=True\n",
    "    time.sleep(100)\n",
    "\n",
    "    while its_there: \n",
    "        try:\n",
    "            count+= 1 \n",
    "            time.sleep(100)\n",
    "            #print(browser.html)\n",
    "            python_button = browser.find_by_text('Load more claims')\n",
    "            python_button.click()\n",
    "            print(f\"Current count {count}\")\n",
    "        except:\n",
    "            its_there=False\n",
    "            #parse data using the 'claim-row' tag- each claim row has one claim with all the data\n",
    "            soup = bs(browser.html, 'html.parser')\n",
    "            #save data into preliminary list\n",
    "            raw_claims_list = soup.find_all(\"div\", class_='claim-row')\n",
    "            print(len(raw_claims_list))\n",
    "            print(\"Scrape Complete\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def myClick(by, desc):\n",
    "#    wait = WebDriverWait(dr, 10)\n",
    "#    by = by.upper()\n",
    "#    if by == 'XPATH':\n",
    "#        wait.until(EC.element_to_be_clickable((By.XPATH, desc))).click()\n",
    "#    if by == 'ID':\n",
    "#        wait.until(EC.element_to_be_clickable((By.ID, desc))).click()\n",
    "#    if by == 'LINK_TEXT':\n",
    "#        wait.until(EC.element_to_be_clickable((By.LINK_TEXT, desc))).click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-67-b57376b32134>, line 26)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-67-b57376b32134>\"\u001b[1;36m, line \u001b[1;32m26\u001b[0m\n\u001b[1;33m    EC.element_to_be_clickable((By.XPATH, \"[@id=\"claims-list\"]/div[51]/button\")))\u001b[0m\n\u001b[1;37m                                                      ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "#from splinter import Browser\n",
    "#import time\n",
    "#from bs4 import BeautifulSoup as bs\n",
    "\n",
    "#print(\"Starting Scrape\")   \n",
    "##click the \"load more claims button until we reach the end of the data\"\n",
    "#executable_path = {'executable_path': 'chromedriver'}\n",
    "#browser = Browser('chrome', **executable_path, headless=False)\n",
    "\n",
    "#browser.visit(\"https://www.washingtonpost.com/graphics/politics/trump-claims-database/\")\n",
    "#count=0\n",
    "#its_there=True\n",
    "#time.sleep(100)\n",
    "\n",
    "#while its_there: \n",
    "#    try:\n",
    "#        count+= 1 \n",
    "#        #time.sleep(200)\n",
    "        #print(browser.html)\n",
    "        #python_button = WebDriverWait(driver, 20).until(\n",
    "         #   EC.presence_of_element_located((By., \"myElement\")))\n",
    "        #python_button = browser.find_by_text('Load more claims')\n",
    "        #wait.until(EC.element_to_be_clickable((By.LINK_TEXT, 'Load more claims'))).click()\n",
    "        #python_button.click()\n",
    "#        element = WebDriverWait(driver, 20).until(\n",
    "#            EC.element_to_be_clickable((By.XPATH, \"[@id=\"claims-list\"]/div[51]/button\")))\n",
    " #       element.click()\n",
    "#        print(f\"Current count {count}\")\n",
    "#    except:\n",
    "#        its_there=False\n",
    "        #parse data using the 'claim-row' tag- each claim row has one claim with all the data\n",
    "#        soup = bs(browser.html, 'html.parser')\n",
    "        #save data into preliminary list\n",
    "#        raw_claims_list = soup.find_all(\"div\", class_='claim-row')\n",
    "#        print(len(raw_claims_list))\n",
    " #       print(\"Scrape Complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'total_claims_list' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-0d79551996a4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#declare a list to hold scraped data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mtotal_claims_list\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'total_claims_list' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "#declare a list to hold scraped data\n",
    "total_claims_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-1-813ebb82f4eb>, line 7)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-1-813ebb82f4eb>\"\u001b[1;36m, line \u001b[1;32m7\u001b[0m\n\u001b[1;33m    while driver.find_elements_by_css_selector('.pg-button' and class = 'pg-button'):\u001b[0m\n\u001b[1;37m                                                                    ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "#url = \"https://www.washingtonpost.com/graphics/politics/trump-claims-database/\"\n",
    "#driver = webdriver.Chrome()\n",
    "#driver.get(url)\n",
    "#html = driver.page_source.encode('utf-8')\n",
    "#page_num = 0\n",
    "\n",
    "#while driver.find_elements_by_css_selector('.pg-button' and class = 'pg-button'):\n",
    "#    driver.find_element_by_css_selector('.pg-button').click()\n",
    "#    page_num += 1\n",
    "#    print(\"getting page number \"+str(page_num))\n",
    "#    time.sleep(3)\n",
    "\n",
    "#html = driver.page_source.encode('utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-44-6e4d0366c5e6>, line 15)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-44-6e4d0366c5e6>\"\u001b[1;36m, line \u001b[1;32m15\u001b[0m\n\u001b[1;33m    count=count +=1\u001b[0m\n\u001b[1;37m                 ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "##Need to click the \"Load more claims\" button until all the data is on the page, and then parse with bs\n",
    " \n",
    "#print(\"Starting Scrape\")   \n",
    "#click the \"load more claims button until we reach the end of the data\"\n",
    "#driver = webdriver.Chrome()\n",
    "#driver.maximize_window()\n",
    "#driver.get(\"https://www.washingtonpost.com/graphics/politics/trump-claims-database/\")\n",
    "\n",
    "\n",
    "#count=0\n",
    "#its_there=True\n",
    "\n",
    "#while its_there: \n",
    "#    try:\n",
    "#        count+= 1\n",
    " #       time.sleep(4)\n",
    "        \n",
    "        # in this example we use x = 253, y = 584\n",
    "        # remember: y = 584 -> it will move down  (a negative value moves up)\n",
    "#        zero_elem = driver.find_element_by_tag_name('button')\n",
    "#        x_body_offset = zero_elem.location[\"x\"]\n",
    "#        y_body_offset = zero_elem.location[\"y\"]\n",
    "#        print(\"Body coordinates: {}, {}\".format(x_body_offset, y_body_offset))\n",
    "\n",
    " #       x = 163.99\n",
    " #       y = 44\n",
    "\n",
    " #       actions = ActionChains(driver)\n",
    " #       actions.move_to_element_with_offset(driver.find_element_by_tag_name('button'), -x_body_offset, -y_body_offset).click()\n",
    " #       actions.move_by_offset( x, y ).click().perform()\n",
    " #       print(count)\n",
    " #   except:\n",
    " #      its_there=False\n",
    "        \n",
    "        #parse data using the 'claim-row' tag- each claim row has one claim with all the data\n",
    "  #     soup = bs(html, 'html.parser')\n",
    "\n",
    "        #save data into preliminary list\n",
    "  #      raw_claims_list = soup.find_all(\"div\", class_='claim-row')\n",
    "  #      print(\"Scrape Complete\")\n",
    "        \n",
    "########################################################################################################################\n",
    "##      CODE DUMP\n",
    "########################################################################################################################\n",
    "\n",
    "#from selenium import webdriver\n",
    "#button = driver.find_element((By.TAG_NAME, \"pg-button\"), href=True)\n",
    "#while button:\n",
    "\n",
    "#button = driver.findElement(By.className(\"pg-button\"))       \n",
    "        #browser.find_element_by_xpath(html/body/article/div[1]/div/div[8]/div/div[51]/button).click()\n",
    "        \n",
    "        #elements = driver.find_elements_by_class_name(\"pg-button\")\n",
    "        #button = elements[0]\n",
    "        #button.click()\n",
    "        \n",
    "        #driver.switch_to.active_element.get_type(\"button.pg-button\").click()\n",
    "\n",
    "#button_next = True\n",
    "#index=0\n",
    "#attr = driver.switch_to.active_element.get_attribute(\"title\")\n",
    "#for i in range(25):\n",
    "    \n",
    "#        this_page_claims_list = []\n",
    "#        index=index+1\n",
    "#        print(index)\n",
    "#        html = browser.html\n",
    "#        time.sleep(.1)\n",
    "#elements = driver.find_elements_by_class_name(\"pg-button\").click()\n",
    "#button = elements[0]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#<button type=\"button\" class=\"pg-button\">Load more claims</button>\n",
    "# //*[@id=\"claims-list\"]/div[51]/button\n",
    "# <button type=\"button\" class=\"pg-button\">Load more claims</button>\n",
    "# /html/body/article/div[1]/div/div[8]/div/div[51]/button\n",
    "\n",
    "#<button type=\"button\" class=\"pg-button\">Load more claims</button>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'WebDriver' object has no attribute 'find_by_text'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-72-8303328f5bc9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;31m# in this example we use x = 253, y = 584\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;31m# remember: y = 584 -> it will move down  (a negative value moves up)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[0mzero_elem\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdriver\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind_by_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Load more claims'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[0mx_body_offset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mzero_elem\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlocation\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"x\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[0my_body_offset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mzero_elem\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlocation\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"y\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'WebDriver' object has no attribute 'find_by_text'"
     ]
    }
   ],
   "source": [
    "driver = webdriver.Chrome()\n",
    "driver.maximize_window()\n",
    "url = \"https://www.washingtonpost.com/graphics/politics/trump-claims-database/\"\n",
    "driver.get(url)\n",
    "# \n",
    "# in this example we use x = 253, y = 584\n",
    "# remember: y = 584 -> it will move down  (a negative value moves up)\n",
    "zero_elem = driver.find_by_text('Load more claims')\n",
    "x_body_offset = zero_elem.location[\"x\"]\n",
    "y_body_offset = zero_elem.location[\"y\"]\n",
    "print(\"Body coordinates: {}, {}\".format(x_body_offset, y_body_offset))\n",
    "\n",
    "x = 163.99\n",
    "y = 44\n",
    "\n",
    "actions = ActionChains(driver)\n",
    "actions.move_to_element_with_offset(driver.find_element_by_tag_name('button'), -x_body_offset, -y_body_offset).click()\n",
    "actions.move_by_offset( x, y ).click().perform()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#parse data using the 'claim-row' tag- each claim row has one claim with all the data\n",
    "soup = bs(html, 'html.parser')\n",
    "\n",
    "#save data into preliminary list\n",
    "raw_claims_list = soup.find_all(\"div\", class_='claim-row')\n",
    "\n",
    "\n",
    "\n",
    "#        total_claims_list.append(this_page_claims_list)\n",
    "#        print(this_page_claims_list[(index*50)-1].prettify())\n",
    "        #browser.find_element(By.TAG_NAME, 'pg-button').click()\n",
    "#        browser.forward()\n",
    "#    browser.click_link_by_partial_text('Load more claims')\n",
    "#        browser.find_elements_by_tag_name('\"pg-button\"')\n",
    "#        button_next = driver.find_element_by_id('pg-button')\n",
    "#        button.click()\n",
    "#    browser.find_element_by_xpath(\"button[@class='pg-button']\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scrape Complete\n"
     ]
    }
   ],
   "source": [
    "#set the path for chromedriver\n",
    "executable_path = {'executable_path': 'chromedriver'}\n",
    "browser = Browser('chrome', **executable_path, headless=True)\n",
    "\n",
    "# Visit url \n",
    "news_url = \"https://www.washingtonpost.com/graphics/politics/trump-claims-database/js/base.js?c=2754871e7eff8a3208c4fe127e3e4bcb01ada41a-1590961789\"\n",
    "browser.visit(news_url)\n",
    "html = browser.html\n",
    "\n",
    "#parse data using the 'claim-row' tag- each claim row has one claim with all the data\n",
    "soup = bs(html, 'html.parser')\n",
    "article = soup.find(\"div\", class_='claim-row')\n",
    "#save data into preliminary list\n",
    "raw_claims_list = soup.find_all(\"div\", class_='claim-row')\n",
    "print(\"Scrape Complete\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#just to make sure I got what I wanted\n",
    "article\n",
    "\n",
    "\n",
    "#    for title_url in titles_and_urls:\n",
    "#       browser.click_link_by_partial_text('next')\n",
    "#except ElementDoesNotExist:\n",
    "#    print(\"Scraping Complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "list indices must be integers or slices, not Tag",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-fee1a483d871>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput_path\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'w'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mclaim\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mraw_claims_list\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m         \u001b[0mtext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mraw_claims_list\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mclaim\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m: list indices must be integers or slices, not Tag"
     ]
    }
   ],
   "source": [
    "#save raw data just in case\n",
    "import os\n",
    "output_path = os.path.join(\"raw_scrape.txt\")\n",
    "with open(output_path,'w') as text:\n",
    "    for claim in raw_claims_list:\n",
    "        text.write(raw_claims_list[claim])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for script in soup([\"script\", \"style\",\"a\",\"<div id=\\\"bottom\\\" >\"]):\n",
    "#    script.extract()    \n",
    "\n",
    "#text = soup.findAll(text=True)\n",
    "#text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set the variables to look for\n",
    "date=()\n",
    "source=()\n",
    "text=()\n",
    "rating=()\n",
    "topic=()\n",
    "response=()\n",
    "link=()\n",
    "repeat_dates=[]\n",
    "repeat_count=0\n",
    "repeat_info={}\n",
    "claim_index=()\n",
    "index=()\n",
    "#initialize the db\n",
    "claim_df={}\n",
    "count=1\n",
    "\n",
    "#define the database\n",
    "claim_df= pd.DataFrame(claim_df,columns=['index','date','source','rating','topic','text','response','link','repeat_count', 'repeat_dates'])\n",
    "\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "list indices must be integers or slices, not Tag",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-45-f90f8133cae6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mindex\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcount\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mcount\u001b[0m\u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m     \u001b[0mdate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mraw_claims_list\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mclaim\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'span'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclass_\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'label'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[1;31m#   source=rawclaims_list[claim].find\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[0mrating\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mraw_claims_list\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mclaim\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind_all\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'span'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclass_\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'pinocchio'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: list indices must be integers or slices, not Tag"
     ]
    }
   ],
   "source": [
    "#loop through each claims-row, extract the data, and append\n",
    "for claim in raw_claims_list:\n",
    "    row = {}\n",
    "    index=count\n",
    "    count+= 1\n",
    "    date=raw_claims_list[claim].find('span', class_='label').text\n",
    "#   source=rawclaims_list[claim].find\n",
    "    rating=raw_claims_list[claim].find_all('span', class_='pinocchio').count\n",
    "#    topic=raw_claims_list[claim].find('p', \"Topic:\", 'span').text\n",
    "    text=raw_claims_list[claim].find('div', class_='claim').text\n",
    "    response=raw_claims_list[claim].find('div', class_='analysis').text\n",
    "#    link=raw_claims_list[claim].find('span', )\n",
    "    repeat_count=raw_claims_list[claim].find('span', class_='underline--green').value\n",
    "    repeat_dates=raw_claims_list[claim].find_all('div', class_='repeat pg-highlight').text\n",
    "\n",
    "    #make a row for the data\n",
    "    \n",
    "    row.index=index\n",
    "    row.date = date\n",
    "    row.source= source\n",
    "    row.raiting=rating\n",
    "    row.topic=topic\n",
    "    row.text=text\n",
    "    row.response=response\n",
    "    row.link=link\n",
    "    row.repeat_count = repeat_count\n",
    "    row.repeat_dates=repeat_dates\n",
    "    \n",
    "    #add row to the claim db   \n",
    "    claim_df.append(row)\n",
    "    \n",
    "\n",
    "#news_title = article.find(\"div\", class_=\"content_title\").text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save data- again, just in case! \n",
    "claim_df.to_csv('claims_data.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
